{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ba38593",
   "metadata": {},
   "source": [
    "\n",
    "The three most popular methods for combining the predictions from different models are:\n",
    "\n",
    "Bagging. Building multiple models (typically of the same type) from different subsamples of the training dataset.\n",
    "\n",
    "Boosting. Building multiple models (typically of the same type) each of which learns to fix the prediction errors of a prior model in the chain.\n",
    "\n",
    "Voting. Building multiple models (typically of differing types) and simple statistics (like calculating the mean) are used to combine predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b9652f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd503472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"C:/Users/Parshuram/Downloads/diabetes.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61c554d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 768 entries, 0 to 767\n",
      "Data columns (total 9 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Pregnancies               768 non-null    int64  \n",
      " 1   Glucose                   768 non-null    int64  \n",
      " 2   BloodPressure             768 non-null    int64  \n",
      " 3   SkinThickness             768 non-null    int64  \n",
      " 4   Insulin                   768 non-null    int64  \n",
      " 5   BMI                       768 non-null    float64\n",
      " 6   DiabetesPedigreeFunction  768 non-null    float64\n",
      " 7   Age                       768 non-null    int64  \n",
      " 8   Outcome                   768 non-null    int64  \n",
      "dtypes: float64(2), int64(7)\n",
      "memory usage: 54.1 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d114c57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.845052</td>\n",
       "      <td>120.894531</td>\n",
       "      <td>69.105469</td>\n",
       "      <td>20.536458</td>\n",
       "      <td>79.799479</td>\n",
       "      <td>31.992578</td>\n",
       "      <td>0.471876</td>\n",
       "      <td>33.240885</td>\n",
       "      <td>0.348958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.369578</td>\n",
       "      <td>31.972618</td>\n",
       "      <td>19.355807</td>\n",
       "      <td>15.952218</td>\n",
       "      <td>115.244002</td>\n",
       "      <td>7.884160</td>\n",
       "      <td>0.331329</td>\n",
       "      <td>11.760232</td>\n",
       "      <td>0.476951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.300000</td>\n",
       "      <td>0.243750</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.372500</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>140.250000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>127.250000</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>0.626250</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>846.000000</td>\n",
       "      <td>67.100000</td>\n",
       "      <td>2.420000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   768.000000  768.000000     768.000000     768.000000  768.000000   \n",
       "mean      3.845052  120.894531      69.105469      20.536458   79.799479   \n",
       "std       3.369578   31.972618      19.355807      15.952218  115.244002   \n",
       "min       0.000000    0.000000       0.000000       0.000000    0.000000   \n",
       "25%       1.000000   99.000000      62.000000       0.000000    0.000000   \n",
       "50%       3.000000  117.000000      72.000000      23.000000   30.500000   \n",
       "75%       6.000000  140.250000      80.000000      32.000000  127.250000   \n",
       "max      17.000000  199.000000     122.000000      99.000000  846.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age     Outcome  \n",
       "count  768.000000                768.000000  768.000000  768.000000  \n",
       "mean    31.992578                  0.471876   33.240885    0.348958  \n",
       "std      7.884160                  0.331329   11.760232    0.476951  \n",
       "min      0.000000                  0.078000   21.000000    0.000000  \n",
       "25%     27.300000                  0.243750   24.000000    0.000000  \n",
       "50%     32.000000                  0.372500   29.000000    0.000000  \n",
       "75%     36.600000                  0.626250   41.000000    1.000000  \n",
       "max     67.100000                  2.420000   81.000000    1.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9860aa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n",
       "       'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecc995d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================\n",
      "Pregnancies ==> Missing zeros : 111\n",
      "============================================\n",
      "Glucose ==> Missing zeros : 5\n",
      "============================================\n",
      "BloodPressure ==> Missing zeros : 35\n",
      "============================================\n",
      "SkinThickness ==> Missing zeros : 227\n",
      "============================================\n",
      "Insulin ==> Missing zeros : 374\n",
      "============================================\n",
      "BMI ==> Missing zeros : 11\n",
      "============================================\n",
      "DiabetesPedigreeFunction ==> Missing zeros : 0\n",
      "============================================\n",
      "Age ==> Missing zeros : 0\n"
     ]
    }
   ],
   "source": [
    "# How many missing zeros are mising in each feature\n",
    "feature_columns = [\n",
    "    'Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', \n",
    "    'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age'\n",
    "]\n",
    "\n",
    "for column in feature_columns:\n",
    "    print(\"============================================\")\n",
    "    print(f\"{column} ==> Missing zeros : {len(df.loc[df[column] == 0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12d49315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================\n",
      "Pregnancies ==> Missing zeros : 0\n",
      "============================================\n",
      "Glucose ==> Missing zeros : 0\n",
      "============================================\n",
      "BloodPressure ==> Missing zeros : 0\n",
      "============================================\n",
      "SkinThickness ==> Missing zeros : 0\n",
      "============================================\n",
      "Insulin ==> Missing zeros : 0\n",
      "============================================\n",
      "BMI ==> Missing zeros : 0\n",
      "============================================\n",
      "DiabetesPedigreeFunction ==> Missing zeros : 0\n",
      "============================================\n",
      "Age ==> Missing zeros : 0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "fill_values = SimpleImputer(missing_values=0, strategy=\"mean\", copy=False)\n",
    "df[feature_columns] = fill_values.fit_transform(df[feature_columns])\n",
    "\n",
    "for column in feature_columns:\n",
    "    print(\"============================================\")\n",
    "    print(f\"{column} ==> Missing zeros : {len(df.loc[df[column] == 0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2010b25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df[feature_columns]\n",
    "y = df.Outcome\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6408d182",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "\n",
    "def evaluate(model, X_train, X_test, y_train, y_test):\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "\n",
    "    print(\"TRAINIG RESULTS: \\n===============================\")\n",
    "    clf_report = pd.DataFrame(classification_report(y_train, y_train_pred, output_dict=True))\n",
    "    print(f\"CONFUSION MATRIX:\\n{confusion_matrix(y_train, y_train_pred)}\")\n",
    "    print(f\"ACCURACY SCORE:\\n{accuracy_score(y_train, y_train_pred):.4f}\")\n",
    "    print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n",
    "\n",
    "    print(\"TESTING RESULTS: \\n===============================\")\n",
    "    clf_report = pd.DataFrame(classification_report(y_test, y_test_pred, output_dict=True))\n",
    "    print(f\"CONFUSION MATRIX:\\n{confusion_matrix(y_test, y_test_pred)}\")\n",
    "    print(f\"ACCURACY SCORE:\\n{accuracy_score(y_test, y_test_pred):.4f}\")\n",
    "    print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de928fec",
   "metadata": {},
   "source": [
    "Bagging Algorithms\n",
    "\n",
    "Bootstrap Aggregation or bagging involves taking multiple samples from your training dataset (with replacement) and training a model for each sample.\n",
    "\n",
    "The final output prediction is averaged across the predictions of all of the sub-models.\n",
    "\n",
    "The three bagging models covered in this section are as follows:\n",
    "\n",
    "Bagged Decision Trees\n",
    "\n",
    "Random Forest\n",
    "\n",
    "Extra Trees\n",
    "\n",
    "\n",
    "1. Bagged Decision Trees\n",
    "\n",
    "Bagging performs best with algorithms that have high variance. A popular example is decision trees, often constructed without pruning.\n",
    "\n",
    "\n",
    "BaggingClassifier:\n",
    "\n",
    "A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregates their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b318505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINIG RESULTS: \n",
      "===============================\n",
      "CONFUSION MATRIX:\n",
      "[[349   0]\n",
      " [  0 188]]\n",
      "ACCURACY SCORE:\n",
      "1.0000\n",
      "CLASSIFICATION REPORT:\n",
      "               0      1  accuracy  macro avg  weighted avg\n",
      "precision    1.0    1.0       1.0        1.0           1.0\n",
      "recall       1.0    1.0       1.0        1.0           1.0\n",
      "f1-score     1.0    1.0       1.0        1.0           1.0\n",
      "support    349.0  188.0       1.0      537.0         537.0\n",
      "TESTING RESULTS: \n",
      "===============================\n",
      "CONFUSION MATRIX:\n",
      "[[119  32]\n",
      " [ 24  56]]\n",
      "ACCURACY SCORE:\n",
      "0.7576\n",
      "CLASSIFICATION REPORT:\n",
      "                    0          1  accuracy   macro avg  weighted avg\n",
      "precision    0.832168   0.636364  0.757576    0.734266      0.764357\n",
      "recall       0.788079   0.700000  0.757576    0.744040      0.757576\n",
      "f1-score     0.809524   0.666667  0.757576    0.738095      0.760049\n",
      "support    151.000000  80.000000  0.757576  231.000000    231.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "bagging_clf = BaggingClassifier(base_estimator=tree, n_estimators=1500, random_state=42)\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "\n",
    "evaluate(bagging_clf, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b26e582",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {\n",
    "    'Bagging Classifier': {\n",
    "        'Train': accuracy_score(y_train, bagging_clf.predict(X_train)),\n",
    "        'Test': accuracy_score(y_test, bagging_clf.predict(X_test)),\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0faa81",
   "metadata": {},
   "source": [
    "2. Random Forest\n",
    "\n",
    "\n",
    "A random forest is a meta-estimator that fits several decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n",
    "\n",
    "The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6133169c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINIG RESULTS: \n",
      "===============================\n",
      "CONFUSION MATRIX:\n",
      "[[349   0]\n",
      " [  0 188]]\n",
      "ACCURACY SCORE:\n",
      "1.0000\n",
      "CLASSIFICATION REPORT:\n",
      "               0      1  accuracy  macro avg  weighted avg\n",
      "precision    1.0    1.0       1.0        1.0           1.0\n",
      "recall       1.0    1.0       1.0        1.0           1.0\n",
      "f1-score     1.0    1.0       1.0        1.0           1.0\n",
      "support    349.0  188.0       1.0      537.0         537.0\n",
      "TESTING RESULTS: \n",
      "===============================\n",
      "CONFUSION MATRIX:\n",
      "[[123  28]\n",
      " [ 29  51]]\n",
      "ACCURACY SCORE:\n",
      "0.7532\n",
      "CLASSIFICATION REPORT:\n",
      "                    0          1  accuracy   macro avg  weighted avg\n",
      "precision    0.809211   0.645570  0.753247    0.727390      0.752538\n",
      "recall       0.814570   0.637500  0.753247    0.726035      0.753247\n",
      "f1-score     0.811881   0.641509  0.753247    0.726695      0.752878\n",
      "support    151.000000  80.000000  0.753247  231.000000    231.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_clf = RandomForestClassifier(random_state=42, n_estimators=1000)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "evaluate(rf_clf, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15bf1d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores['Random Forest'] = {\n",
    "        'Train': accuracy_score(y_train, rf_clf.predict(X_train)),\n",
    "        'Test': accuracy_score(y_test, rf_clf.predict(X_test)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983e56b5",
   "metadata": {},
   "source": [
    "3. Extra Trees\n",
    "\n",
    "Extra Trees are another modification of bagging where random trees are constructed from samples of the training dataset.\n",
    "\n",
    "You can construct an Extra Trees model for classification using the ExtraTreesClassifier class.\n",
    "\n",
    "\n",
    "ExtraTreeClassifier:\n",
    "\n",
    "This class implements a meta-estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84b89030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINIG RESULTS: \n",
      "===============================\n",
      "CONFUSION MATRIX:\n",
      "[[349   0]\n",
      " [  0 188]]\n",
      "ACCURACY SCORE:\n",
      "1.0000\n",
      "CLASSIFICATION REPORT:\n",
      "               0      1  accuracy  macro avg  weighted avg\n",
      "precision    1.0    1.0       1.0        1.0           1.0\n",
      "recall       1.0    1.0       1.0        1.0           1.0\n",
      "f1-score     1.0    1.0       1.0        1.0           1.0\n",
      "support    349.0  188.0       1.0      537.0         537.0\n",
      "TESTING RESULTS: \n",
      "===============================\n",
      "CONFUSION MATRIX:\n",
      "[[124  27]\n",
      " [ 25  55]]\n",
      "ACCURACY SCORE:\n",
      "0.7749\n",
      "CLASSIFICATION REPORT:\n",
      "                    0          1  accuracy   macro avg  weighted avg\n",
      "precision    0.832215   0.670732  0.774892    0.751473      0.776290\n",
      "recall       0.821192   0.687500  0.774892    0.754346      0.774892\n",
      "f1-score     0.826667   0.679012  0.774892    0.752840      0.775531\n",
      "support    151.000000  80.000000  0.774892  231.000000    231.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "ex_tree_clf = ExtraTreesClassifier(n_estimators=1000, max_features=7, random_state=42)\n",
    "ex_tree_clf.fit(X_train, y_train)\n",
    "evaluate(ex_tree_clf, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9466bea6",
   "metadata": {},
   "source": [
    "Boosting Algorithms\n",
    "\n",
    "Boosting ensemble algorithms creates a sequence of models that attempt to correct the mistakes of the models before them in the sequence.\n",
    "\n",
    "Once created, the models make predictions that may be weighted by their demonstrated accuracy and the results are combined to create a final output prediction.\n",
    "\n",
    "The two most common boosting ensemble machine learning algorithms are:\n",
    "\n",
    "AdaBoost\n",
    "\n",
    "Stochastic Gradient Boosting\n",
    "\n",
    "\n",
    "1. AdaBoost\n",
    "\n",
    "AdaBoost was perhaps the first successful boosting ensemble algorithm. It generally works by weighting instances in the dataset by how easy or difficult they are to classify, allowing the algorithm to pay less attention to them in the construction of subsequent models.\n",
    "\n",
    "You can construct an AdaBoost model for classification using the AdaBoostClassifier class.\n",
    "\n",
    "\n",
    "AdaBoostClassifier:\n",
    "\n",
    "An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9b2bea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINIG RESULTS: \n",
      "===============================\n",
      "CONFUSION MATRIX:\n",
      "[[310  39]\n",
      " [ 51 137]]\n",
      "ACCURACY SCORE:\n",
      "0.8324\n",
      "CLASSIFICATION REPORT:\n",
      "                    0           1  accuracy   macro avg  weighted avg\n",
      "precision    0.858726    0.778409  0.832402    0.818567      0.830607\n",
      "recall       0.888252    0.728723  0.832402    0.808488      0.832402\n",
      "f1-score     0.873239    0.752747  0.832402    0.812993      0.831056\n",
      "support    349.000000  188.000000  0.832402  537.000000    537.000000\n",
      "TESTING RESULTS: \n",
      "===============================\n",
      "CONFUSION MATRIX:\n",
      "[[123  28]\n",
      " [ 27  53]]\n",
      "ACCURACY SCORE:\n",
      "0.7619\n",
      "CLASSIFICATION REPORT:\n",
      "                    0          1  accuracy   macro avg  weighted avg\n",
      "precision    0.820000   0.654321  0.761905    0.737160      0.762622\n",
      "recall       0.814570   0.662500  0.761905    0.738535      0.761905\n",
      "f1-score     0.817276   0.658385  0.761905    0.737830      0.762249\n",
      "support    151.000000  80.000000  0.761905  231.000000    231.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_boost_clf = AdaBoostClassifier(n_estimators=30)\n",
    "ada_boost_clf.fit(X_train, y_train)\n",
    "evaluate(ada_boost_clf, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "094f8814",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores['AdaBoost'] = {\n",
    "        'Train': accuracy_score(y_train, ada_boost_clf.predict(X_train)),\n",
    "        'Test': accuracy_score(y_test, ada_boost_clf.predict(X_test)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e12a57",
   "metadata": {},
   "source": [
    "2. Stochastic Gradient Boosting\n",
    "\n",
    "Stochastic Gradient Boosting (also called Gradient Boosting Machines) is one of the most sophisticated ensemble techniques. It is also a technique that is proving to be perhaps of the best techniques available for improving performance via ensembles.\n",
    "\n",
    "\n",
    "GradientBoostingClassifier:\n",
    "\n",
    "GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b518207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINIG RESULTS: \n",
      "===============================\n",
      "CONFUSION MATRIX:\n",
      "[[342   7]\n",
      " [ 19 169]]\n",
      "ACCURACY SCORE:\n",
      "0.9516\n",
      "CLASSIFICATION REPORT:\n",
      "                    0           1  accuracy   macro avg  weighted avg\n",
      "precision    0.947368    0.960227  0.951583    0.953798      0.951870\n",
      "recall       0.979943    0.898936  0.951583    0.939439      0.951583\n",
      "f1-score     0.963380    0.928571  0.951583    0.945976      0.951194\n",
      "support    349.000000  188.000000  0.951583  537.000000    537.000000\n",
      "TESTING RESULTS: \n",
      "===============================\n",
      "CONFUSION MATRIX:\n",
      "[[116  35]\n",
      " [ 26  54]]\n",
      "ACCURACY SCORE:\n",
      "0.7359\n",
      "CLASSIFICATION REPORT:\n",
      "                    0          1  accuracy   macro avg  weighted avg\n",
      "precision    0.816901   0.606742  0.735931    0.711821      0.744119\n",
      "recall       0.768212   0.675000  0.735931    0.721606      0.735931\n",
      "f1-score     0.791809   0.639053  0.735931    0.715431      0.738906\n",
      "support    151.000000  80.000000  0.735931  231.000000    231.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "grad_boost_clf = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "grad_boost_clf.fit(X_train, y_train)\n",
    "evaluate(grad_boost_clf, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b00a87",
   "metadata": {},
   "source": [
    "Voting Ensemble\n",
    "\n",
    "Voting is one of the simplest ways of combining predictions from multiple machine learning algorithms.\n",
    "\n",
    "It works by first creating two or more standalone models from your training dataset. A Voting Classifier can then be used to wrap your models and average the predictions of the sub-models when asked to make predictions for new data.\n",
    "\n",
    "The predictions of the sub-models can be weighted, but specifying the weights for classifiers manually or even heuristically is difficult. More advanced methods can learn how to best weight the predictions from submodels, but this is called stacking (stacked generalization) and is currently not provided in Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6de207af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINIG RESULTS: \n",
      "===============================\n",
      "CONFUSION MATRIX:\n",
      "[[327  22]\n",
      " [ 82 106]]\n",
      "ACCURACY SCORE:\n",
      "0.8063\n",
      "CLASSIFICATION REPORT:\n",
      "                    0           1  accuracy   macro avg  weighted avg\n",
      "precision    0.799511    0.828125  0.806331    0.813818      0.809529\n",
      "recall       0.936963    0.563830  0.806331    0.750396      0.806331\n",
      "f1-score     0.862797    0.670886  0.806331    0.766841      0.795610\n",
      "support    349.000000  188.000000  0.806331  537.000000    537.000000\n",
      "TESTING RESULTS: \n",
      "===============================\n",
      "CONFUSION MATRIX:\n",
      "[[130  21]\n",
      " [ 35  45]]\n",
      "ACCURACY SCORE:\n",
      "0.7576\n",
      "CLASSIFICATION REPORT:\n",
      "                    0          1  accuracy   macro avg  weighted avg\n",
      "precision    0.787879   0.681818  0.757576    0.734848      0.751148\n",
      "recall       0.860927   0.562500  0.757576    0.711714      0.757576\n",
      "f1-score     0.822785   0.616438  0.757576    0.719612      0.751323\n",
      "support    151.000000  80.000000  0.757576  231.000000    231.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "estimators = []\n",
    "log_reg = LogisticRegression(solver='liblinear')\n",
    "estimators.append(('Logistic', log_reg))\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "estimators.append(('Tree', tree))\n",
    "\n",
    "svm_clf = SVC(gamma='scale')\n",
    "estimators.append(('SVM', svm_clf))\n",
    "\n",
    "voting = VotingClassifier(estimators=estimators)\n",
    "voting.fit(X_train, y_train)\n",
    "\n",
    "evaluate(voting, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8188fcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores['Voting'] = {\n",
    "        'Train': accuracy_score(y_train, voting.predict(X_train)),\n",
    "        'Test': accuracy_score(y_test, voting.predict(X_test)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42afd5eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores_df = pd.DataFrame(scores)\n",
    "\n",
    "scores_df.plot(kind='barh', figsize=(15, 8))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
